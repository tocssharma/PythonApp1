{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYQVGPe5RLV4"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIy1K8LWRLV5"
      },
      "source": [
        "Placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZbSppohRLV5"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQC4t2KsRLV5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth and dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# Install other specific dependencies without their dependencies (as per original code intent)\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft bitsandbytes\n",
        "# Install accelerate separately to ensure a compatible version and resolve its dependencies\n",
        "!pip install accelerate>=0.29.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "hmuTyZ40cpfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment for Google Colab file upload\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# dataset_path = list(uploaded.keys())[0]\n",
        "\n",
        "# For Kaggle, update this path after uploading\n",
        "#dataset_path = \"/kaggle/input/your-dataset/finetuning_dataset.jsonl\"\n",
        "dataset_path = \"/finetuning_dataset.jsonl\"\n",
        "\n",
        "\n",
        "print(f\"Dataset path: {dataset_path}\")"
      ],
      "metadata": {
        "id": "q1GukpIUdVNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "print(f\"âœ“ Dataset loaded: {len(dataset)} examples\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(dataset[0]['text'][:500] + \"...\")\n",
        "\n",
        "# Optional: Split into train/validation (80/20)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"\\nâœ“ Split complete:\")\n",
        "print(f\"  Training examples: {len(train_dataset)}\")\n",
        "print(f\"  Validation examples: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "5icuNDp2hk_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "805j0Ftvhj4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "max_seq_length = 2048  # Choose any! Unsloth auto-supports RoPE Scaling internally\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "\n",
        "# Choose your model - small models for quick training\n",
        "# Options:\n",
        "# - \"unsloth/Qwen2.5-0.5B-Instruct\"  # 0.5B params - Very fast\n",
        "# - \"unsloth/Qwen2.5-1.5B-Instruct\"  # 1.5B params - Good balance\n",
        "# - \"unsloth/Llama-3.2-1B-Instruct\"  # 1B params - Good quality\n",
        "# - \"unsloth/Mistral-7B-v0.3\"        # 7B params - Better quality, slower\n",
        "\n",
        "model_name = \"unsloth/Qwen3-1.7B\"\n",
        "\n",
        "print(f\"Selected model: {model_name}\")\n",
        "print(f\"Max sequence length: {max_seq_length}\")\n",
        "print(f\"4-bit quantization: {load_in_4bit}\")"
      ],
      "metadata": {
        "id": "lybEqgnwcweW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./outputs\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=5,\n",
        "    optim=\"adamw_torch\",  # Changed from adamw_8bit\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=5,\n",
        "    report_to=[],\n",
        "    fp16=True,\n",
        "    seed=3407,\n",
        "    remove_unused_columns=False,  # KEY FIX\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training arguments configured\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Total epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {
        "id": "p7Lv5qk_hsm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable all external logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"true\"\n",
        "\n",
        "print(\"âœ“ All external logging disabled\")"
      ],
      "metadata": {
        "id": "IkXxFqbLkBCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Model loaded successfully!\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "iJxG8v8aizqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - higher = more capacity but slower (8, 16, 32, 64)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,  # LoRA scaling factor\n",
        "    lora_dropout=0,  # Dropout for LoRA layers (0 = no dropout)\n",
        "    bias=\"none\",  # Bias training (\"none\", \"all\", \"lora_only\")\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Longer training but less memory\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"âœ“ LoRA configuration applied!\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M\")"
      ],
      "metadata": {
        "id": "dQ60Tq6vihb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",  # Column name with text data\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Can make training 5x faster for short sequences\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Trainer created successfully!\")"
      ],
      "metadata": {
        "id": "jW-Xbw80hy7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable all external logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"true\"\n",
        "\n",
        "print(\"âœ“ All external logging disabled\")"
      ],
      "metadata": {
        "id": "Djnt4zFzkTyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show GPU memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU: {gpu_stats.name}\")\n",
        "print(f\"GPU memory: {start_gpu_memory} GB / {max_memory} GB\")\n",
        "print(\"\\nðŸš€ Starting training...\\n\")\n",
        "\n",
        "print(\"âœ“ All external logging disabled\")\n",
        "# Start training\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Show final stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")\n",
        "print(f\"\\nFinal GPU memory: {used_memory} GB\")\n",
        "print(f\"Memory used for training: {used_memory_for_lora} GB\")\n",
        "print(f\"Percentage of GPU used: {used_percentage}%\")\n",
        "print(f\"\\nTraining time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")"
      ],
      "metadata": {
        "id": "S6iDRIMCh2kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test query\n",
        "test_query = \"Show me teams with high utilization\"\n",
        "\n",
        "# Format prompt\n",
        "prompt = f\"\"\"### Instruction:\n",
        "Generate SQL and a visualization for the user.\n",
        "\n",
        "### User Query:\n",
        "{test_query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(\"Test Query:\", test_query)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(response.split(\"### Response:\")[1].strip())\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "aw7oNkDch8Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\n",
        "    \"Find members with low availability\",\n",
        "    \"Show sprint progress for all teams\",\n",
        "    \"Which teams have pending work exceeding capacity?\",\n",
        "    \"Calculate average completion rate by work item type\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "Generate SQL and a visualization for the user.\n",
        "\n",
        "### User Query:\n",
        "{query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, use_cache=True)\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Test #{i}: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    print(response.split(\"### Response:\")[1].strip())\n",
        "    print()"
      ],
      "metadata": {
        "id": "nnkQFTi0h_4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Save LoRA adapters only (smallest)\n",
        "model.save_pretrained(\"sql_viz_lora\")\n",
        "tokenizer.save_pretrained(\"sql_viz_lora\")\n",
        "print(\"âœ“ LoRA adapters saved to: sql_viz_lora/\")"
      ],
      "metadata": {
        "id": "sXmO1Roqg35j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Save merged model (16-bit)\n",
        "model.save_pretrained_merged(\"sql_viz_model_16bit\", tokenizer, save_method=\"merged_16bit\")\n",
        "print(\"âœ“ Merged 16-bit model saved to: sql_viz_model_16bit/\")"
      ],
      "metadata": {
        "id": "famudue2g4fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 3: Save quantized model for Ollama (Q4_K_M format)\n",
        "model.save_pretrained_gguf(\"sql_viz_model_Q4\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "model.save_pretrained_gguf(\"sql_viz_model_NQ4\", tokenizer\")\n",
        "print(\"âœ“ GGUF model saved to: sql_viz_model-Q4_K_M.gguf\")\n",
        "print(\"  You can use this with Ollama or llama.cpp!\")"
      ],
      "metadata": {
        "id": "9cyDB4cMg6dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colab - create a zip file\n",
        "# !zip -r sql_viz_model.zip sql_viz_lora/\n",
        "# from google.colab import files\n",
        " files.download('sql_viz_model.zip')"
      ],
      "metadata": {
        "id": "2nZkfP06g_4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}